{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit Flair Detector.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radonys/Reddit-Flair-Detector/blob/master/Reddit_Flair_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CTIVuFgiuoAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reddit Flair Detector\n",
        "\n",
        "A Reddit Flair Detector using Machine Learning Algorithms"
      ]
    },
    {
      "metadata": {
        "id": "ByC5RpkXvVNY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install Required Modules"
      ]
    },
    {
      "metadata": {
        "id": "LZVX18CvvTUI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install praw pandas numpy scikit-learn matplotlib scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQ5D5ZVHxuCW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Modules"
      ]
    },
    {
      "metadata": {
        "id": "n2Zx5yYjxxwg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import datetime as dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FUPnSWuedSF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "metadata": {
        "id": "jLYT1shRdVgY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_date(created):\n",
        "    return dt.datetime.fromtimestamp(created)\n",
        "  \n",
        "def append_values(topics_dict, flair, submission):\n",
        "  \n",
        "  topics_dict[\"flair\"].append(flair)\n",
        "  topics_dict[\"title\"].append(submission.title)\n",
        "  topics_dict[\"score\"].append(submission.score)\n",
        "  topics_dict[\"id\"].append(submission.id)\n",
        "  topics_dict[\"url\"].append(submission.permalink)\n",
        "  topics_dict[\"comms_num\"].append(submission.num_comments)\n",
        "  topics_dict[\"created\"].append(submission.created)\n",
        "  topics_dict[\"body\"].append(submission.selftext)\n",
        "  topics_dict[\"author\"].append(submission.author)\n",
        "  topics_dict[\"comments\"].append(submission.comments)\n",
        "  \n",
        "def save_data_csv(topics_dict, subset):\n",
        "  \n",
        "  topics_data = pd.DataFrame(train_dict)\n",
        "  _timestamp = topics_data[\"created\"].apply(get_date)\n",
        "  topics_data = topics_data.assign(timestamp = _timestamp)\n",
        "  del topics_data[\"created\"]\n",
        "  topics_data.to_csv(subset+\".csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wi9NhpmXyxd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Getting Reddit Data\n",
        "\n",
        "Using PRAW module [1]"
      ]
    },
    {
      "metadata": {
        "id": "juhA_Mxiy59Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(client_id='_4Z0SOGEI31a3Q', client_secret='fzhJE6_cjPqLN6goG4R7cJ6SK3Q', user_agent='reddit-flair-detector', username='radonys', password='Yash@1234')\n",
        "\n",
        "subreddit = reddit.subreddit('india')\n",
        "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\"]\n",
        "train_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"comms_num\": [], \"created\": [], \"body\":[], \"author\":[], \"comments\":[]}\n",
        "test_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"comms_num\": [], \"created\": [], \"body\":[], \"author\":[], \"comments\":[]}\n",
        "\n",
        "for flair in flairs:\n",
        "  \n",
        "  get_subreddits = subreddit.search(flair, limit=50)\n",
        "  train_split_ratio = 0.8 * 50 #20% Testing Data\n",
        "  counter = 0\n",
        "  \n",
        "  for submission in get_subreddits:\n",
        "    \n",
        "    counter += 1\n",
        "    \n",
        "    if counter <= train_split_ratio:\n",
        "      append_values(train_dict, flair, submission)\n",
        "      \n",
        "    else:\n",
        "      append_values(test_dict, flair, submission)\n",
        "    \n",
        "save_data_csv(train_dict, \"train\")\n",
        "save_data_csv(test_dict, \"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m3LQb-vv7G9c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "\n",
        "1) http://www.storybench.org/how-to-scrape-reddit-with-python/"
      ]
    }
  ]
}